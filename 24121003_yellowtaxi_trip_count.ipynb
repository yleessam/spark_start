{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59723906-2263-49df-894a-4bc78207eeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/10 16:38:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"24121003_yellowtaxi_trip_count\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfd0c988-e4ba-444a-b820-6f96857f8dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "trip_files = '/trips/*'\n",
    "zone_file = 'taxi+_zone_lookup.csv'\n",
    "directory = os.path.join(os.getcwd(), 'data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef4fb06b-626b-4dc0-b4f7-690a38f4d1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trips_df = spark.read.csv(f'file:///{directory}/{trip_files}', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81339099-5a54-4ed3-9292-c26c0f3f76b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_df = spark.read.csv(f'file:///{directory}/{zone_file}', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ede9fd52-1d18-49bf-93a4-cdcb541d0159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trips_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28a41b51-9da3-417e-a5b1-fd80d3ab4aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df.createOrReplaceTempView('trips')\n",
    "zone_df.createOrReplaceTempView('zone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0ba9b2f-4120-4659-a90e-4852e80a1922",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "select\n",
    "t.VendorID,\n",
    "TO_DATE(t.tpep_pickup_datetime) as pickup_date,\n",
    "TO_DATE(t.tpep_dropoff_datetime) as dropoff_date,\n",
    "HOUR(t.tpep_pickup_datetime)  as pickup_time,\n",
    "HOUR(t.tpep_dropoff_datetime) as dropoff_time,\n",
    "t.passenger_count,\n",
    "t.trip_distance,\n",
    "t.tip_amount,\n",
    "t.total_amount,\n",
    "t.payment_type,\n",
    "pz.Zone as pickup_zone,\n",
    "dz.Zone as dropoff_zone\n",
    "from trips t\n",
    "LEFT JOIN zone pz ON t.PULocationID = pz.LocationID\n",
    "LEFT JOIN zone dz ON t.DOLocationID = dz.LocationID\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "342a8753-d9d3-4743-b2ff-d503a44f0944",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_df = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48872857-5b21-48a2-a8c3-1b40eb7db443",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "485d3575-bdbf-41d4-b34c-65e32c411b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+------------+-----------+------------+---------------+-------------+----------+------------+------------+-----------------+--------------+\n",
      "|VendorID|pickup_date|dropoff_date|pickup_time|dropoff_time|passenger_count|trip_distance|tip_amount|total_amount|payment_type|      pickup_zone|  dropoff_zone|\n",
      "+--------+-----------+------------+-----------+------------+---------------+-------------+----------+------------+------------+-----------------+--------------+\n",
      "|       2| 2021-03-01|  2021-03-01|          0|           0|              1|          0.0|       0.0|         4.3|           2|               NV|            NV|\n",
      "|       2| 2021-03-01|  2021-03-01|          0|           0|              1|          0.0|       0.0|         3.8|           2|   Manhattanville|Manhattanville|\n",
      "|       2| 2021-03-01|  2021-03-01|          0|           0|              1|          0.0|       0.0|         4.8|           2|   Manhattanville|Manhattanville|\n",
      "|       1| 2021-03-01|  2021-03-01|          0|           0|              0|         16.5|     11.65|       70.07|           1|LaGuardia Airport|            NA|\n",
      "|       2| 2021-03-01|  2021-03-01|          0|           0|              1|         1.13|      1.86|       11.16|           1|     East Chelsea|            NV|\n",
      "+--------+-----------+------------+-----------+------------+---------------+-------------+----------+------------+------------+-----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comb_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91c19481-9d74-4a1b-ad01-6b9fa957c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_df.createOrReplaceTempView('comb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34bb6a5d-de61-48db-b002-558b32e82c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "select pickup_date, pickup_time \n",
    "from comb \n",
    "where pickup_time>0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40e0f4e1-85d3-487a-8bb8-284699066dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|pickup_date|pickup_time|\n",
      "+-----------+-----------+\n",
      "| 2021-02-28|         23|\n",
      "| 2021-02-28|         23|\n",
      "| 2021-02-28|         23|\n",
      "| 2021-02-28|         23|\n",
      "| 2021-02-28|         23|\n",
      "| 2021-02-28|         23|\n",
      "| 2021-02-28|         23|\n",
      "| 2021-03-01|         22|\n",
      "| 2021-03-01|          1|\n",
      "| 2021-03-01|          1|\n",
      "| 2021-03-01|          1|\n",
      "| 2021-03-01|          1|\n",
      "| 2021-03-01|          1|\n",
      "| 2021-03-01|          1|\n",
      "| 2021-03-01|          1|\n",
      "| 2021-03-01|          1|\n",
      "| 2021-03-01|          1|\n",
      "| 2021-03-01|          1|\n",
      "| 2021-03-01|          1|\n",
      "| 2021-03-01|          1|\n",
      "+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b8744e7-4d25-4556-add6-3a42ddb89d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|pickup_date|pickup_time|\n",
      "+-----------+-----------+\n",
      "| 2009-01-01|          0|\n",
      "| 2008-12-31|         23|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|          1|\n",
      "| 2009-01-01|          0|\n",
      "| 2008-12-31|         23|\n",
      "| 2008-12-31|         23|\n",
      "| 2008-12-31|         23|\n",
      "| 2008-12-31|         23|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|         16|\n",
      "| 2009-01-01|         16|\n",
      "| 2009-01-01|          0|\n",
      "| 2009-01-01|          0|\n",
      "+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "select pickup_date, pickup_time \n",
    "from comb \n",
    "where pickup_date<'2020-12-31'\n",
    "'''\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cabf067-3bd9-4ff9-9a3a-c9bbc62ba428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, VendorID: string, pickup_time: string, dropoff_time: string, passenger_count: string, trip_distance: string, tip_amount: string, total_amount: string, payment_type: string, pickup_zone: string, dropoff_zone: string]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f7d8d66-95c9-485a-8f37-6324aabf95cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Project [cast(tpep_pickup_datetime#17 as date) AS pickup_date#76, hour(cast(tpep_pickup_datetime#17 as timestamp), Some(Asia/Seoul)) AS pickup_time#78]\n",
      "+- *(3) BroadcastHashJoin [DOLocationID#24], [LocationID#82], LeftOuter, BuildRight, false\n",
      "   :- *(3) Project [tpep_pickup_datetime#17, DOLocationID#24]\n",
      "   :  +- *(3) BroadcastHashJoin [PULocationID#23], [LocationID#68], LeftOuter, BuildRight, false\n",
      "   :     :- *(3) Filter (isnotnull(tpep_pickup_datetime#17) AND (cast(tpep_pickup_datetime#17 as date) < 18627))\n",
      "   :     :  +- FileScan csv [tpep_pickup_datetime#17,PULocationID#23,DOLocationID#24] Batched: false, DataFilters: [isnotnull(tpep_pickup_datetime#17), (cast(tpep_pickup_datetime#17 as date) < 18627)], Format: CSV, Location: InMemoryFileIndex[file:/home/tutor/src/data/trips/.ipynb_checkpoints, file:/home/tutor/src/data/t..., PartitionFilters: [], PushedFilters: [IsNotNull(tpep_pickup_datetime)], ReadSchema: struct<tpep_pickup_datetime:string,PULocationID:int,DOLocationID:int>\n",
      "   :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#272]\n",
      "   :        +- *(1) Filter isnotnull(LocationID#68)\n",
      "   :           +- FileScan csv [LocationID#68] Batched: false, DataFilters: [isnotnull(LocationID#68)], Format: CSV, Location: InMemoryFileIndex[file:/home/tutor/src/data/taxi+_zone_lookup.csv], PartitionFilters: [], PushedFilters: [IsNotNull(LocationID)], ReadSchema: struct<LocationID:int>\n",
      "   +- ReusedExchange [LocationID#82], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#272]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select pickup_date, pickup_time \n",
    "# from comb \n",
    "# where pickup_date<'2020-12-31'\n",
    "spark.sql(query).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fe0c103-3624-4188-9676-cf21c6c8c5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Project [cast(tpep_pickup_datetime#17 as date) AS pickup_date#76, hour(cast(tpep_pickup_datetime#17 as timestamp), Some(Asia/Seoul)) AS pickup_time#78]\n",
      "+- *(3) BroadcastHashJoin [DOLocationID#24], [LocationID#82], LeftOuter, BuildRight, false\n",
      "   :- *(3) Project [tpep_pickup_datetime#17, DOLocationID#24]\n",
      "   :  +- *(3) BroadcastHashJoin [PULocationID#23], [LocationID#68], LeftOuter, BuildRight, false\n",
      "   :     :- *(3) Filter ((isnotnull(tpep_pickup_datetime#17) AND (hour(cast(tpep_pickup_datetime#17 as timestamp), Some(Asia/Seoul)) > 0)) AND (hour(cast(tpep_pickup_datetime#17 as timestamp), Some(Asia/Seoul)) <= 12))\n",
      "   :     :  +- FileScan csv [tpep_pickup_datetime#17,PULocationID#23,DOLocationID#24] Batched: false, DataFilters: [isnotnull(tpep_pickup_datetime#17), (hour(cast(tpep_pickup_datetime#17 as timestamp), Some(Asia/..., Format: CSV, Location: InMemoryFileIndex[file:/home/tutor/src/data/trips/.ipynb_checkpoints, file:/home/tutor/src/data/t..., PartitionFilters: [], PushedFilters: [IsNotNull(tpep_pickup_datetime)], ReadSchema: struct<tpep_pickup_datetime:string,PULocationID:int,DOLocationID:int>\n",
      "   :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#356]\n",
      "   :        +- *(1) Filter isnotnull(LocationID#68)\n",
      "   :           +- FileScan csv [LocationID#68] Batched: false, DataFilters: [isnotnull(LocationID#68)], Format: CSV, Location: InMemoryFileIndex[file:/home/tutor/src/data/taxi+_zone_lookup.csv], PartitionFilters: [], PushedFilters: [IsNotNull(LocationID)], ReadSchema: struct<LocationID:int>\n",
      "   +- ReusedExchange [LocationID#82], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#356]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#실행계획, 실행결과(4040)\n",
    "\n",
    "query2 = '''\n",
    "select pickup_date, pickup_time \n",
    "from comb \n",
    "where pickup_time > 0 and pickup_time<=12\n",
    "'''\n",
    "spark.sql(query2).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "721ab1df-f7a2-4045-8a60-0bd995f29a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Sort [pickup_date#76 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(pickup_date#76 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#504]\n",
      "   +- *(4) HashAggregate(keys=[pickup_date#76], functions=[count(1)])\n",
      "      +- Exchange hashpartitioning(pickup_date#76, 200), ENSURE_REQUIREMENTS, [id=#500]\n",
      "         +- *(3) HashAggregate(keys=[pickup_date#76], functions=[partial_count(1)])\n",
      "            +- *(3) Project [cast(tpep_pickup_datetime#17 as date) AS pickup_date#76]\n",
      "               +- *(3) BroadcastHashJoin [DOLocationID#24], [LocationID#82], LeftOuter, BuildRight, false\n",
      "                  :- *(3) Project [tpep_pickup_datetime#17, DOLocationID#24]\n",
      "                  :  +- *(3) BroadcastHashJoin [PULocationID#23], [LocationID#68], LeftOuter, BuildRight, false\n",
      "                  :     :- *(3) Filter (isnotnull(tpep_pickup_datetime#17) AND (hour(cast(tpep_pickup_datetime#17 as timestamp), Some(Asia/Seoul)) > 0))\n",
      "                  :     :  +- FileScan csv [tpep_pickup_datetime#17,PULocationID#23,DOLocationID#24] Batched: false, DataFilters: [isnotnull(tpep_pickup_datetime#17), (hour(cast(tpep_pickup_datetime#17 as timestamp), Some(Asia/..., Format: CSV, Location: InMemoryFileIndex[file:/home/tutor/src/data/trips/.ipynb_checkpoints, file:/home/tutor/src/data/t..., PartitionFilters: [], PushedFilters: [IsNotNull(tpep_pickup_datetime)], ReadSchema: struct<tpep_pickup_datetime:string,PULocationID:int,DOLocationID:int>\n",
      "                  :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#461]\n",
      "                  :        +- *(1) Filter isnotnull(LocationID#68)\n",
      "                  :           +- FileScan csv [LocationID#68] Batched: false, DataFilters: [isnotnull(LocationID#68)], Format: CSV, Location: InMemoryFileIndex[file:/home/tutor/src/data/taxi+_zone_lookup.csv], PartitionFilters: [], PushedFilters: [IsNotNull(LocationID)], ReadSchema: struct<LocationID:int>\n",
      "                  +- ReusedExchange [LocationID#82], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#461]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query3 = '''\n",
    "select pickup_date , count(*) as trip_count\n",
    "from comb \n",
    "where pickup_time > 0\n",
    "group by pickup_date\n",
    "order by pickup_date\n",
    "'''\n",
    "spark.sql(query3).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c3debd7-4123-4e70-8b61-86ab5a5ceb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(spark_start)",
   "language": "python",
   "name": "spark_start"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
